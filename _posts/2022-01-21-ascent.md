# Gradient Ascent

_This entry journals the initial substantial progress made in the direction of demystifying machine learning starting from January'21 to May'21._

Suppose you want to accomplish something that no one has achieved yet. It is like scaling a cliff whose peaks are obscured behind dark clouds. Many have trekked up to different heights. They have taken different treacherous paths. Some proclaim to know the way and keep fantasizing that we shall reach the peak just after we trek this little curb. It is just right around the corner!

<p align="center">
  <img width="500" src="https://www.companyofpainters.com/wp-content/uploads/2016/04/Caspar_David_Friedrich_-_Wanderer_above_the_sea_of_fog-600x768.jpg" alt="zarathustra">
</p>

You find yourself in the town below the cliff. In the hustle-bustle, you find some young, a bit wiser, folk who’ve scaled some distance up to the mountain. Some of them are enthusiastic to share the path they’ve taken, however minor their progress might have been. Some prefer to conceal their secret trails. They’ve all taken different routes, but you don’t have all the time in the world. You can only heed the advice of the one you felt knew what they were talking about, the initial step, all on your gut feeling.

My first steps towards the peak of AGI were motivated by Manu S Pillai's [talk](https://www.youtube.com/watch?v=yqeajyG774w) on How to get started with Machine Learning". It was January 2nd when I took notes from his session. Finally, I had some clarity on how I could achieve my dream with a roadmap. I believe in dreaming up a great goal on a time scale of a lifetime and then breaking it down into smaller goals that can be achieved in a few months. I can't say whether this is the right approach, we'll know by the time I die :D.

### Jan 2021: Introduction to basic terms
Watching the lectures by Andrew Ng on ML was a euphoric experience. The idea that a machine can learn any mapping between inputs and outputs given sufficient data was just fascinating. I learned linear regression, logistic regression or classification, and neural networks. I made notes and solved the quizzes that came along with the lectures. I also solved the programming assignments given in Octave. Octave is a language similar to MATLAB for the purpose of scientific computing.

It all went well until week five's programming assignment involving coding-up backpropagation from scratch. Backpropagation is an algorithm through which neural networks improve themselves, and it is a bedrock of deep learning. I understood the mathematical explanation of backprop but was clueless when it was time to code it. Gave it a few attempts and made no progress. I realized that my coding chops were not as top-notch as I thought they were. Unfortunately, began to procrastinate solving the assignment. In fact, I am still procrastinating solving that assignment to this day :P. Although I did write code for backprop in python later down the line.

Whenever I stumble on something difficult, I can convince myself easily that I am just not ready now. I will be able to do it after I learn something tangential! I do not give up. I will never accept that I cannot do something. If not today, someday I will be capable of doing it! Is it just a defence mechanism? Or a genuine tactic for handling challenging problems?

### Feb - Mar 2021: Learning the tools
Hitting a wall with Octave, I decided to do ML in python. At this point, it had almost been 2 months since I wrote any python code. So I went out to refresh my skills on charming this snake called Python. As I recall, refreshing python was an unorganized ordeal. Sentdex, Corey Schafer, Kunal Kushwaha et al were some of the guys whose tutorials on python I watched. Alongside the tutorials, I also practised from HackerRank. Also picked up a book by Jake VanderPlas called "A whirlwind tour of Python", which was pretty helpful in crystallizing all of Python in one place.

For learning the libraries required for ML such as Numpy, pandas and matplotlib, Jake VanderPlas again came to the rescue with his Python Data Science Handbook. I read its ipython and Numpy section and found it to be the best material presented on these topics. I highly recommend the talk on Numpy by Jake. I also probably watched many random tutorials on youtube, out of which Kunal Kushwaha's ML Bootcamp deserves a special mention. The Bootcamp has been unfortunately deleted from youtube due to some drama among its creators.

During this period, there were sessions given by seniors of JMI for introducing the freshers to tech. They were part of the HASHes coding club. One of these sessions was on ML by Azhan. That session further gave clarity on how I should be learning ML. He also recommended reading the Hands-On Machine Learning book.

The next step, therefore, was to read the Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow book by Aurelion Geron. Numpy, pandas and matplotlib were actually prerequisites to reading this book. The book was already on the reading list as per the roadmap laid out by Manu. Hence, however random the progress within these months felt, in the end, it was all part of the plan.

### Apr 2021: Familiarizing with the tradition
I had tried reading the book back in January, just after the Andrew Ng course, and found it hard to follow since I did not know the prerequisites. Now, being done with learning the prerequisites, I blazed through the first few chapters within a month. Algorithms like linear regression, logistic regression, SVMs, decision trees and random forests started to make some sense. I read through the book and replicated the code in Jupyter notebooks to ensure I wasn't cruising through a novel.

### May 2021: First Illumination
After learning through the traditional supervised ML algorithms, I wasn't satisfied. We surely can't achieve AGI with SVMs or random forests. Therefore I decided to do the Deep Learning specialization on Coursera by Andrew Ng. I went onto Coursera and found that there was a free 7-day trial for the specialization. The specialization consisted of 5 courses:
Neural Networks & Deep Learning
Improving Deep NN: Hyperparameter tuning, regularization and optimization
Structuring your ML projects
Convolutional Neural Networks (CNN)
Sequence Models & NLP
So starting from 3rd May, I tried speedrunning through the courses to gain as many certificates as possible in the 7-day free trial period. In the 7-days I successfully completed the lectures with the assignments of the first three courses. I also made detailed notes along the way. It was all hands on deck for a week. Was it the right thing to do? I'd say probably not since I didn't retain much afterwards. Was it fun? Hell yeah! That week was a thrilling experience unravelling the mystery of neural networks. I wanted to satiate my curiosity, and the course really did live up to that mark. After that week, I could confidently feel that Neural networks were not magic and can appreciate the potential it has for achieving AGI. 

Speedrunning through courses might be fun, but it is not productive in the long run. Ideally, as per the specialization creators, it should have taken a month to complete a course. I have realized now that gaining skills and satiating your curiosity are two different things. You can read up on novel ideas very quickly, but can't retain them. Honestly, most of my average day consists of watching or reading up on novel ideas. Channels like Vsauce, Veritasium, smarter everyday, Tedx, etc., have probably sucked a huge portion out of my life. I do not know if I regret it. Sometimes I wonder whether I actually want to work on AGI, or I'm just curious to learn about it.

On May 21st, I turned 20. Felt weird to realize that I've entered my 20s. Also, didn't feel like I had much of a teenage experience. Most of May was spent on the anticipation and some reflection on what awaits ahead. Being anxious about your future is the most 'engineer' thing to do :).



 


